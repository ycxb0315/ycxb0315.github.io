---
layout:     post
title:      项目中遇到的问题总结
subtitle:   hadoop、spark、kafka等大数据组件经典问题总结
date:       2020-02-10
author:     Yao
header-img: /img/post-bg-keybord.jpg
catalog: true
tags:
    - 大数据
	- flume
	- hadoop
	- yarn
	- spark
	- kafka
	- maxwell
	- hbase
	- zookeeper
---



# 项目中典型问题总结

## 一、flume相关

### 问题一：kafaka无法获取到CDH版本flume的数据

这部分可能是由于CDH启动的flume的进程问题，解决办法是手动重启flume
```
#杀死flume的命令，命令多操作几次
 ps -ef|grep flume|awk -F ' ' '{print $2}'|xargs kill -9 

#重启flume
nohup /opt/cloudera/parcels/CDH-5.14.2-1.cdh5.14.2.p0.3/bin/flume-ng  agent --name a1 --conf-file /opt/cm-5.14.2/run/cloudera-scm-agent/process/159-flume-AGENT/flume.conf &
```
注意：上面process路径后的flume版本号根据实际的主机来确定，一般选择数字较大的版本，一般为最新

## 二、scala相关
### 1、java的map转scala的map
在scala的编码过程，会有部分代码调用的是Java代码，便会出现数据类型转换的问题。最常见的解决办法是
```
import scala.collection.JavaConversions._
```
使用隐式转换进行转换，如果需要特定指定某个包，可以如下方法导入
(1)、Java Map转换Scala Map：
```
import scala.collection.JavaConversions.mapAsJavaMap
```
(2)、Scala Map转换Java Map:
```
import scala.collection.JavaConversions.mapAsScalaMap
```
### 2、JavaConversions和JavaConverters

这两个对象非常相似，但这两个有什么不同呢？

- JavaConversions 提供了一系列隐式方法，可以在Java集合和最接近的相应Scala集合之间进行转换，反之亦然 . 这是通过创建实现Scala接口的包装器并将调用转发到底层Java集合或Java接口，将调用转发到底层Scala集合来完成的 .

- JavaConverters 使用pimp-my-library模式将 asScala 方法“添加”到Java集合中，并将 asJava 方法添加到Scala集合中，这些方法返回上面讨论的相应包装器 . 它比 JavaConversions （自2.8以来）更新（自版本2.8.1起）并使Scala和Java集合之间的转换显式化 . 与大卫在他的回答中所写的相反，我建议你习惯使用 JavaConverters ，因为你编写代码会产生很多隐式转换的可能性要大得多，因为你可以控制那个会产生很多隐式转换的代码 . 发生：你在哪里写 .asScala 或 .asJava  .

### 3、阿里巴巴的fastJson在scala中使用出错
在使用fastJson对scala的mutable的map进行序列化的时候，会报下面的错误
```
Error:(108, 47) ambiguous reference to overloaded definition,
both method toJSONString in object JSON of type (x$1: Any, x$2: com.alibaba.fastjson.serializer.SerializerFeature*)String
and  method toJSONString in object JSON of type (x$1: Any)String
match argument types (org.apache.spark.sql.entity.LineageParseResult) and expected result type String
    val response = new Request(url).body(JSON.toJSONString(lineageParseResult)).POST

```
#### 原因解释 
scala与java编译器重载逻辑不匹配。Java在编译时候，首先会根据实参的数量和类型来进行处理，
根据源码可以看出，有两个toString方法：
```
public static String toJSONString(Object object) {
        return toJSONString(object, emptyFilters);
    }

public static String toJSONString(Object object, SerializerFeature... features) {
        return toJSONString(object, DEFAULT_GENERATE_FEATURE, features);
    }
```
在第二个方法中SerializerFeature... features 是一个可变长参数，带有变长参数的方法重载使得scala在调用方法时感到“模糊”，就无法匹配参数的类型。

#### 解决方法
在将map或者seq转为json的过程中
##### 1、实验失败的方案
在其他的博客当中有人提到用scala显式调用 JSON.toJSONString()方法
```
println(JSON.toJSONString(seq,SerializerFeature.PrettyFormat))
```
得到的结果也不正确
```
{
	"empty":false,
	"traversableAgain":true
}
```
##### 2、未实验的方法
转自：https://blog.csdn.net/k_wzzc/article/details/90032138
创建一个java类，再其中创建一个调用JSON.toJSONString(ll)的方法，然后再由scala调用该方法即可，这样就可以避免scala的模糊调用。
```
import com.alibaba.fastjson.JSON;
import java.util.List;
public class Seq2Json<E> {

    public String seq2Josn(List<E> ll) {
        String string = JSON.toJSONString(ll);
        return string;
    }

}
```
```
val json = new Seq2Json[String]
println(json.seq2Josn(seq2)
```
##### 3、本人项目中使用的方法
导入scala的json包
```
import scala.util.parsing.json.JSONObject
```
转换的过程
```
JSONObject(columnsValueMap.toMap).toString()
```
其中的columnsValueMap是mutable.HashMap类型的
```
 val columnsValueMap = mutable.HashMap.empty[String, String]
```
## 三、Spark相关
### 序列化问题
如果没有实现序列化就会出现spark任务序列化失败不能正常运行，本人在项目中有这样一段代码
```
val jedis = JedisUtil.getInstance().getJedis
val rdd:RDD[Row] = data.rdd
def readyPut(row:Row) = {
	...
	//写入redis
	jedis.set(rk,JSONObject(columnsValueMap.toMap).toString())
	//      (rk, JSONObject(columnsValueMap.toMap).toString())
}
//写入redis
rdd.foreach(readyPut)
```
此时代码出现如下错误
```
Exception in thread "main" org.apache.spark.SparkException: Task not serializable
```
可以使用@Transient  和 implicit去解决，如果不生效，可以将jedis放入readyPut内部，如下：
```
val rdd:RDD[Row] = data.rdd
def readyPut(row:Row) = {
	...
	//写入redis
	val jedis = JedisUtil.getInstance().getJedis
	jedis.set(rk,JSONObject(columnsValueMap.toMap).toString())
	//      (rk, JSONObject(columnsValueMap.toMap).toString())
}
//写入redis
rdd.foreach(readyPut)
```
## IDEA快捷键

中国加油，武汉加油